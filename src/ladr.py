import os, logging, ladr, filemgt, commands, process
#from ClifModuleSet import *

def translate_to_tptp_file (input_file, output_file, symbols = None):
    # hack: replace iff's (<->) by <- to do a correct windows translation; revert it afterwards
    replaced = substitute_iffs(input_file)
    
    print "translating " + input_file + " using " + str(symbols)
    
    # convert LADR file to lower case and surround all symbols by "", only temporarily for TPTP translation
    file = open(input_file, 'r')
    text = file.readlines()
    file.close()
    temp_file = input_file +'.tmp'
    file = open(temp_file, 'w')
    text = [line.replace('(',' ( ') for line in text]
    text = [line.replace(')',' ) ') for line in text]
    symbols = sorted([sym.strip() for sym in symbols], key=lambda s: len(s), reverse=True)
    # for the LADR to TPTP translation to work properly, we need upper-case symbols.
    # However, then the names are all destroyed afterwards -> need a better way of dealing with this.
    for sym in symbols:
        text = [line.replace(' '+sym+' ',' '+sym.upper()+' ') for line in text]
        #text = [line.replace(' '+sym+' ',' "'+sym + '" ') for line in text]
    #text = [line.strip().lower() for line in text]
    newtext = []
    for s in text:
        if len(s)>0: newtext.append(s+'\n')
    newtext = comment_imports(newtext)
    file.writelines(newtext)
    file.close()
    
    cmd = commands.get_ladr_to_tptp_cmd(temp_file, output_file)
    
    process.executeSubprocess(cmd)  

    #delete temp file
    if os.path.exists(temp_file) and os.path.isfile(temp_file):
        os.remove(temp_file)

    number_tptp_axioms(output_file)

    # complete hack
    if replaced: restore_iffs(output_file)

    #if symbols:
    #    get_lowercase_tptp_file(output_file, symbols)

    logging.getLogger(__name__).info("CREATED TPTP TRANSLATION: " + output_file)
    
    return output_file



def cumulate_ladr_files (input_files, output_file):    
    """write all axioms from a set of p9 files to a single file without any change in the content itself except for the replacement of certain symbols"""
    special_symbols = filemgt.get_tptp_symbols()
    
    logging.getLogger(__name__).debug("Special symbols: " + str(special_symbols))
    
    filemgt.read_config('converters','clif-to-prover9')
    
    text = []
    for f in input_files:
        in_file = open(f, 'r')
        line = in_file.readline()
        while line:
            if len(special_symbols)>0:
                for key in special_symbols:
                    line = line.replace(' '+key+'(', ' '+special_symbols[key]+'(')
                    line = line.replace('('+key+'(', '('+special_symbols[key]+'(')
            text.append(line)
            line = in_file.readline()
        in_file.close()
    
    text = strip_inner_commands(text)
    text = comment_imports(text)
    
    # store the location of all "<-" to be able to replace them back later on:
   
    file = open(output_file, 'w+')
    file.write('%axioms from module ' + f + ' and all its imports \n')
    file.write('%----------------------------------\n')
    file.write('\n')
    file.writelines(text) 
    file.close()
    return output_file

def number_tptp_axioms (tptp_file):
    """Give each axiom in a TPTP file a unique number and convert CNF lines to FOF lines."""       
    f = open(tptp_file, 'r')
    lines = f.readlines()
    f.close()
    f = open(tptp_file, 'w')
    counter = 1
    for line in lines:
        line = line.replace('cnf(sos,','fof(sos'+str(counter)+',')
        line = line.replace('fof(sos,','fof(sos'+str(counter)+',')
        f.write(line)
        counter += 1
    f.close()
    return tptp_file


     
def get_lowercase_tptp_file (tptp_file, nonlogical_symbols):    
    """"converts all symbols from uppercase to lowercase and removes quotation marks generated by ladr_to_tptp translator.
    This prepares the file for use with Vampire."""

    # sort nonlogical symbols by length (descending) to avoid replacing parts of longer symbols
    nonlogical_symbols = sorted(nonlogical_symbols, key=lambda s: len(s), reverse=True) 
    logging.getLogger(__name__).debug("Nonlogical symbols: " + str(nonlogical_symbols))

    tptp_file = ladr.number_tptp_axioms(tptp_file)

    file = open(tptp_file, 'r')
    text = file.read()
    file.close()
    
    for symbol in nonlogical_symbols:
        print symbol
        text = text.replace('\"'+symbol[0]+'\"', ' '+symbol[0].lower())
    for symbol in nonlogical_symbols:
        text = text.replace(symbol[0], symbol[0].lower())
    for symbol in nonlogical_symbols:
        text = text.replace(symbol[0].upper(), symbol[0].lower())
    out_file = open(tptp_file, 'w+')
    out_file.write(text)
    out_file.close()
    return out_file


def substitute_iffs(ladr_file):
    """ replace all equivalence ("iff" or "<=>" statements) by "<=" to circumvent an error in the ladr_to_tptp translator in Windows."""
    if not os.name == 'nt':
        return
    else: 
        file = open(ladr_file, 'r')
        text = file.readlines()
        file.close()   
        
        # ensure proper spacing
        text = [s.replace("<->"," <iff> ") for s in text]
        replaced = False
    
        if " <iff> " in "".join(text):
            logging.getLogger(__name__).debug("replacing equivalences")
            if "<-" in "".join(text):
                logging.getLogger(__name__).error("Problem converting LADR files to TPTP syntax: cannot deal with <-> properly in Windows due to error in 2007 version of old ladr_to_tptp.")
                return replaced
            else:
                text = [s.replace("<iff>","<-") for s in text]
                replaced = True
    
        file = open(ladr_file, 'w+')
        file.writelines(text) 
        file.close()
        return replaced
    


def restore_iffs(tptp_file):
    """restore equivalences in the TPTP output from "<=" to "<=>"."""
    if not os.name == 'nt':
        return False
    file = open(tptp_file, 'r')
    text = file.readlines()
    file.close()   
    file = open(tptp_file, 'w+')
    file.writelines([s.replace("<=","<=>") for s in text]) 
    file.close()
    return True
    
    
def strip_inner_commands(text):
    text = "".join(text)    # convert list of lines into a single string
    """remove all "formulas(sos)." and "end_of_list." from a p9 file assembled from multiple axiom files; leaving a single block of axioms"""
    parts = text.split("end_of_list.")
    # get the goal clauses
    goals = []
    for i in range(0,len(parts)):
        gparts = parts[i].split("formulas(goals).\n")
        if len(gparts)>2:
            # Problem!!
            raise LadrParsingError("Syntax error in ladr input: mismatch of 'formulas(goals).' and 'end_of_list.' keywords in" + ("".join(gparts)))
        elif len(gparts)==2:
            goals.extend([g.strip() + "\n" for g in gparts[1].split("\n")])
            parts[i] = parts[i].replace(parts[i],"").strip("\n").strip()
    # get the axioms
    axioms = []
    for i in range(0,len(parts)):
        aparts = parts[i].split("formulas(sos).\n")
        if len(aparts)>2:
            # Problem!!
            raise LadrParsingError("Syntax error in ladr input: mismatch of 'formulas(sos).' and 'end_of_list.' keywords in" + ("".join(aparts)))
        elif len(aparts)==2:
            axioms.extend([a.strip() + "\n" for a in aparts[1].split("\n")])
            parts[i] = parts[i].replace(parts[i],"").strip("\n").strip()
        
    # comment remainder
    for p in parts:
        text = ["% "+s.strip() +"\n" for s in p.split("\n")]

    # add axioms and goals
    if len(axioms)>0:
        text.append("formulas(sos).\n")
        text.extend(axioms)
        text.append("end_of_list.\n")
    if len(goals)>0: 
        text.append("formulas(goals).\n")
        text.extend(goals)
        text.append("end_of_list.\n")

    for p in text:
        if p == "%\n":
            text.remove(p)

    #print text
    return text


def comment_imports (lines):
    #print "Commenting imports in LADR file"
    for i in range(0,len(lines)):
        keyword = 'imports('    # this is the syntax used by the clif-to-prover9 converter
        if lines[i].strip().find(keyword) > -1:
            logging.getLogger(__name__).info("module import found: " + lines[i].strip('\n'))
            lines[i] = '% ' + lines[i]
            #new_module_name = line.strip()[len(keyword)+1:-3].strip()
        else:
            pass
    return lines
        
        
def get_ladr_goal_files (lemmas_file,  lemmas_name):
    """break a single lemma file into individual lemma files (with lemmas_name in its file name), each containing a single lemma."""
    sentences = split_lemma_into_sentences(lemmas_file)
    return get_lemma_files_from_sentences(lemmas_name, sentences)


def split_lemma_into_sentences(lemmas_file):
    """take a lemma file in the LADR format and split it into individual goals that can be feed into Prover9."""
    input_file = open(lemmas_file, 'r')
    lines = input_file.readlines()
    input_file.close()
    
    # comment all the imports
    logging.getLogger(__name__).debug("commenting all imports in " + lemmas_file)
    lines = comment_imports(lines)
        
    #print(lines)    
    sentences = []

    started = False
    for line in lines:
        lineparts = line.strip().split('%')
        #print lineparts
        if lineparts[0]:
            lineparts[0] = lineparts[0].strip('\n').strip()
            if len(lineparts[0])>0:
                if not started:
                    if line.find('formulas(sos).') > -1 or line.find('formulas(assumptions).') > -1:
                        started  = True
                else:
                    if line.find('end_of_list.') > -1:
                        break
                    else:
                        sentences.append(lineparts[0])
    
    logging.getLogger(__name__).info("Split " + lemmas_file + " into " + str(len(sentences)) + " individual lemma files")
    return sentences



def get_lemma_files_from_sentences (lemmas_name, sentences):

    sentences_files = []
    sentences_names = []

    import math
    # determine maximal number of digits
    digits = int(math.log10(len(sentences)))+1
    
    i = 1

    for lemma in sentences:
        name = lemmas_name + '_goal' + ('{0:0'+ str(digits) +  'd}').format(i)
        
        filename = filemgt.get_full_path(name, 
                              folder=filemgt.read_config('ladr','folder'), 
                              ending=filemgt.read_config('ladr','ending'))        
        output_file = open(filename, 'w')
        output_file.write('formulas(goals).\n')
        output_file.write(lemma + '\n')
        output_file.write('end_of_list.\n')
        output_file.close()
        sentences_files.append(filename)
        sentences_names.append(name)
        i += 1
    
    return (sentences_names, sentences_files)


class LadrParsingError(Exception):
    
    output = []
    
    def __init__(self, value, output=[]):
        self.value = value
        self.output = output
        logging.getLogger(__name__).error(repr(self.value) + '\n\n' + (''.join('{}: {}'.format(*k) for k in enumerate(self.output))))
    def __str__(self):
        return repr(self.value) + '\n\n' + (''.join('{}: {}'.format(*k) for k in enumerate(self.output)))


